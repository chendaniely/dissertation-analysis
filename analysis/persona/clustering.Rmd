---
title: "Clustering"
output: 
  html_document: 
    toc: yes
    df_print: kable
    fig_width: 8
    fig_height: 5
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5)

library(here)
library(readr)
library(dplyr)
library(tidyr)
library(cluster)
library(purrr)

persona <- readr::read_tsv(here("./data/original/surveys/02-self_assessment_with_questions.tsv"), )

wide <- persona %>%
  dplyr::select(id, question_part, response) %>%
  dplyr::filter(stringr::str_starts(question_part, "Q[3|4|5|6]")) %>%
  tidyr::pivot_wider(names_from = question_part,
                     values_from = response) %>%
  dplyr::select(-starts_with("Q3.2")) %>%
  tidyr::drop_na() %>%
  {.}

# make sure the row names are the same as the IDs before dropping
rownames(wide) <- wide$id
stopifnot(all(rownames(wide) == wide$id))

wide <- dplyr::select(wide, -id)

# should only have the text responses to convert to factors
stopifnot(all(lapply(wide, class) == "character"))
q3.1_unique <- length(wide$Q3.1 %>% unique())
numeric_data <- purrr::map_df(wide, ~as.numeric(as.factor(.)))
stopifnot(length(unique(numeric_data$Q3.1)) == q3.1_unique)
```

# The data

```{r}
head(numeric_data)
```

The data are the numeric (factor) survey responses.
They are not `scale`d, but the units are arbitrary.
Using `scale` would scale/standardize the results.
But these are factor responses, not an actual measurement.
**This needs to be consulted.**

Scaling the data would look like this:

```{r}
scale(numeric_data)[1:5, 1:6]
```



# PCA

```{r}
pca_persona <- prcomp(numeric_data)

summary(pca_persona)
```

Half of the components account for >90% of the variance.
Will need to confirm these results with factor analysis,
but this makes sense since I tried to ask the same questions in 2 ways.

# Hierarchical Clustering

https://uc-r.github.io/hc_clustering

```{r}
# Euclidean distance, drops the first column of IDs
dist <- dist(numeric_data, method = "euclidean")
```

## Finding the clustering method

### Complete

Computes pairwise similarities, and the "most" (i.e., maximum) similar ones are grouped together

```{r}
# Hierarchical Clustering with hclust
hc_complete <- hclust(dist, method = "complete")
# Plot the result
plot(hc_complete, cex = 0.6, hang = -1)
```

### Ward's

Minimizes the total **within cluster** variance.

From the docs:

> Two different algorithms are found in the literature for Ward clustering. The one used by option "ward.D" (equivalent to the only Ward option "ward" in R versions <= 3.0.3) does not implement Ward's (1963) clustering criterion, whereas option "ward.D2" implements that criterion (Murtagh and Legendre 2014). With the latter, the dissimilarities are squared before cluster updating. Note that agnes(*, method="ward") corresponds to hclust(*, "ward.D2").

```{r}
hc_ward <- hclust(dist, method = "ward.D2")
# Plot the result
plot(hc_ward, cex = 0.6, hang = -1)
```


### Comparing dendograms

Calculate agglomerative coefficient (closer to 1 mean stronger clustering structure)

```{r}
m <- c("average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

purrr::map_dbl(m, ~ cluster::agnes(numeric_data, method = .)$ac)
```

Ward's gives us the best clustering structure

```{r}
hc_agnes_wards <- agnes(numeric_data, method = "ward")
cluster::pltree(hc_agnes_wards, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
```

## Cutting the tree to create clusters
