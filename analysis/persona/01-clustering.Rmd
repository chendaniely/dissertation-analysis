---
title: "Clustering"
output: 
  html_document: 
    toc: yes
    df_print: kable
    fig_width: 8
    fig_height: 5
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 5)
options(width = 100)

library(here)
library(readr)
library(dplyr)
library(tidyr)
library(cluster)
library(purrr)
library(factoextra)
library(fs)

persona <- readr::read_tsv(here("./data/original/surveys/02-self_assessment_with_questions.tsv"), )

wide <- persona %>%
  dplyr::select(id, question_part, response) %>%
  dplyr::filter(stringr::str_starts(question_part, "Q[3|4|5|6]")) %>%
  tidyr::pivot_wider(names_from = question_part,
                     values_from = response) %>%
  dplyr::select(-starts_with("Q3.2")) %>%
  tidyr::drop_na() %>%
  {.}

# make sure the row names are the same as the IDs before dropping
wide <- tibble::column_to_rownames(wide, var = "id")
stopifnot(all(rownames(wide) == wide$id))

rownames(wide)

# should only have the text responses to convert to factors
stopifnot(all(lapply(wide, class) == "character"))
q3.1_unique <- length(wide$Q3.1 %>% unique())
numeric_data <- purrr::map_df(wide, ~as.numeric(as.factor(.)))
stopifnot(length(unique(numeric_data$Q3.1)) == q3.1_unique)

rownames(numeric_data) <- rownames(wide)
```

# The data

```{r}
tail(numeric_data)
```

The data are the numeric (factor) survey responses.
They are not `scale`d, but the units are arbitrary.
Using `scale` would scale/standardize the results.
But these are factor responses, not an actual measurement.
**This needs to be consulted.**

Scaling the data would look like this:

```{r}
scale(numeric_data)[1:5, 1:6]
```



# PCA

```{r}
pca_persona <- prcomp(numeric_data)
summary(pca_persona)
```

Half of the components account for >90% of the variance.
Will need to confirm these results with factor analysis,
but this makes sense since I tried to ask the same questions in 2 ways.

# Hierarchical Clustering

https://uc-r.github.io/hc_clustering

```{r}
# Euclidean distance, drops the first column of IDs
dist <- dist(numeric_data, method = "euclidean")
```

## Finding the clustering method

### Complete

Computes pairwise similarities, and the "most" (i.e., maximum) similar ones are grouped together

```{r}
# Hierarchical Clustering with hclust
hc_complete <- hclust(dist, method = "complete")
# Plot the result
plot(hc_complete, cex = 0.6, hang = -1)
```

### Ward's

Minimizes the total **within cluster** variance.

From the docs:

> Two different algorithms are found in the literature for Ward clustering. The one used by option "ward.D" (equivalent to the only Ward option "ward" in R versions <= 3.0.3) does not implement Ward's (1963) clustering criterion, whereas option "ward.D2" implements that criterion (Murtagh and Legendre 2014). With the latter, the dissimilarities are squared before cluster updating. Note that agnes(*, method="ward") corresponds to hclust(*, "ward.D2").

```{r}
hc_ward <- hclust(dist, method = "ward.D2")
# Plot the result
plot(hc_ward, cex = 0.6, hang = -1)
```


### Comparing dendograms

Calculate agglomerative coefficient (closer to 1 mean stronger clustering structure)

```{r}
m <- c("average", "single", "complete", "ward")
names(m) <- c( "average", "single", "complete", "ward")

purrr::map_dbl(m, ~ cluster::agnes(numeric_data, method = .)$ac)
```

Ward's gives us the best clustering structure

```{r}
hc_agnes_wards <- agnes(numeric_data, method = "ward")
cluster::pltree(hc_agnes_wards, cex = 0.6, hang = -1, main = "Dendrogram of agnes")
```

## Cutting the tree to create clusters

```{r}
hc_ward
```

```{r}
sub_grp3 <- cutree(hc_ward, k = 3)
sub_grp4 <- cutree(hc_ward, k = 4)
sub_grp5 <- cutree(hc_ward, k = 5)
sub_grp6 <- cutree(hc_ward, k = 6)
```

```{r}
numeric_data <- numeric_data %>%
  dplyr::mutate(
    group3 = sub_grp3,
    group4 = sub_grp4,
    group5 = sub_grp5,
    group6 = sub_grp6
  )
rownames(numeric_data) <- rownames(wide)
```

```{r}
plot(hc_ward, cex = 0.6)
rect.hclust(hc_ward, k = 3, border = 2:4)
```

```{r}
plot(hc_ward, cex = 0.6)
rect.hclust(hc_ward, k = 4, border = 2:5)
```

```{r}
plot(hc_ward, cex = 0.6)
rect.hclust(hc_ward, k = 5, border = 2:6)
```

```{r}
plot(hc_ward, cex = 0.6)
rect.hclust(hc_ward, k = 6, border = 2:7)
```

### Finding optimal clusters

#### Elbow method

https://uc-r.github.io/kmeans_clustering#elbow

```{r}
factoextra::fviz_nbclust(numeric_data, FUN = factoextra::hcut, method = "wss")
```

Seems like 5 clusters is optimal (where the "elbow" bends).
Will need to look at the results to see if they create meaningful personas


# Write out group data

```{r, include=FALSE}
group_data <- numeric_data %>%
  tibble::rownames_to_column(var = "id") %>%
  dplyr::select(id, group5)

stopifnot(nrow(numeric_data) == nrow(group_data))

stopifnot(all(unique(group_data$id) %in% unique(persona$id)))

persona5 <- persona %>%
  dplyr::mutate(id = as.character(id)) %>%
  dplyr::left_join(group_data, by = c("id" = "id"))

fs::dir_create(here("./data", "final", "persona"), recurse = TRUE)
readr::write_tsv(persona5, here("./data/final/persona/persona_group_5.tsv"))
```
